<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">	  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">	  
  <link rel="icon" type="image/png" href="resource/speedinghzl_logo.jpeg">
  <script type="text/javascript" src="resource/hidebib.js"></script>
  <title>Zilong's Home Page</title>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  </head>
  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>Zilong Huang / 黄子龙</name>
        </p>
        <p> </p>
	<p>I am now a research scientist at ByteDance. 
    I received my Ph.D. degree and B.E. degree from Huazhong University of Science and Technology (HUST) in 2020 and 2015 respectively, advised by <a href="http://eic.hust.edu.cn/professor/liuwenyu/" target="_blank">Prof. Wenyu Liu</a> and <a href="https://xwcv.github.io/" target="_blank">Prof. Xinggang Wang</a>. I was working as a visiting student (2018-2019) in the <a href="http://ifp-uiuc.github.io/" target="_blank">IFP group</a> at the University of Illinois at Urbana-Champaign (UIUC), advised by <a href="http://www.ece.illinois.edu/directory/profile/t-huang1" target="_blank">Prof. Thomas S. Huang</a>, <a href="https://weiyc.github.io/" target="_blank">Prof. Yunchao Wei</a> and <a href="https://www.humphreyshi.com/" target="_blank">Prof. Humphrey Shi</a>.

    <p>I work on computer vision problems with special focus on Multimodal learning, Efficient model design and training.</p>


		<p></p>



        <p align=center>
	  <a href="resource/cv-hzl.pdf">CV</a> &nbsp/&nbsp
          <a href="mailto:zilong.huang2020@gmail.com">Email</a> &nbsp/&nbsp
          <a href="https://github.com/speedinghzl">GitHub</a> &nbsp/&nbsp
          <!--a href="">Thesis</a-->
          <a href="https://scholar.google.com/citations?user=GW9vw8UAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp 
          <a href="https://www.linkedin.com/in/%E5%AD%90%E9%BE%99-%E9%BB%84-b16339ab/"> LinkedIn </a>
	  <!--a href="https://analogicalnexus.github.io/blog/">Blog</a-->
        </p>
        </td>
              <td width="66%">
                <img style="width:100%;max-width:100%" alt="profile photo" src="resource/zilong.jpeg" class="round_icon"></a>
              </td>
      </tr>
      </table>
<!-- News -->
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

        <tbody><tr><td>

            <heading>Highlights</heading>
            <ul>
             
    <p> <span class="red-bold ">[Call for paper]</span> Image and Vision Computing <a href="https://www.sciencedirect.com/journal/image-and-vision-computing/about/call-for-papers"> Special Issue on "Advancing Transparency and Privacy: Explainable AI and Synthetic Data in Biometrics and Computer Vision" </a></p>
    <li>[Oct 2024] <a href="https://arxiv.org/abs/2411.03313">SuperClass</a> and <a href="https://depth-anything-v2.github.io"> Depth-Anything V2 </a> are accepted by NeurIPS 2024.</li>
    <li>[Sep 2024] Ranked as <a href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw/7" target="_blank">The World’s Top 2% Scientists 2024 </a>  by Stanford University/Elsevier. </li>
    <li>[May 2024] Co-organizing the ECCV 2024 Workshop <a href="https://syntheticdata4cv.wordpress.com/"> Synthetic Data for Computer Vision </a></li>
    <li>[Feb 2024] <a href="https://github.com/LiheYoung/Depth-Anything"> Depth-Anything </a> is accepted by CVPR 2024.</li>
    <li>[Dec 2023] <a href="https://github.com/fudan-zvg/meta-prompts">Meta Prompts </a> is released. </li>
    <li>[Oct 2023] Ranked as <a href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw/6" target="_blank">The World’s Top 2% Scientists 2023 </a>  by Stanford University/Elsevier. </li>
    <li>[Nov 2023] <a href="https://arxiv.org/abs/2304.00784" target="_blank">Disentangled Pre-training for Image Matting</a> is accepted by WACV 2024 as Oral</li>
    <li>[Feb 2023] <a href="https://arxiv.org/abs/2212.04048" target="_blank">Motion Latent-based Diffusion</a> is accepted by CVPR 2023</li>
    <li>[Jan 2023] <a href="https://arxiv.org/abs/2301.13156" target="_blank">SeaFormer</a> is accepted by ICLR 2023 and <a href="https://ieeexplore.ieee.org/document/10025770" target="_blank">DCNet</a> is accepted by TCSVT </li>
		<li>[Sep 2022] <a href="https://arxiv.org/abs/2210.11170" target="_blank">COCO-Nerf</a> is accepted by NeurIPS 2022</li>	
		<li>[Mar 2022] <a href="https://arxiv.org/abs/2204.05525" target="_blank">Topformer</a>  is accepted by CVPR 2022</li>
		<li>[Dec 2021] Delighted to receive <a href="http://www.csig.org.cn/detail/3233" target="_blank">CSIG Excellent Doctoral Dissertation Award</a></li>
		<li>[May 2021] <a href="https://arxiv.org/abs/2003.00872" target="_blank">AlignSeg</a>  is accepted by TPAMI</li>
    <li>[May 2021] Selected as <a href="https://xueshu.baidu.com/usercenter/index/detail?tab_id=4&id=361" target="_blank">Global Top 100 Chinese Rising Stars in Artificial Intelligence</a> by Baidu Scholar</li>
		<li>[Mar 2021] Win two champions on <a href="https://arxiv.org/pdf/2105.08630.pdf" target="_blank">Single Image Depth Estimation Challenge of Mobile AI 2021</a> and  <a href="https://maadaa.ai/cvpr2021-short-video-face-parsing-challenge/" target="_blank">Short-video Face Parsing Challenge of The 3rd Person in Context (PIC) Workshop</a></li>        
            </ul>
        </td></tr>      
	    </table>


<!-- Research -->

	  
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Publications</heading>
	  <div id="publication" style="width: 100%; margin-top: 10px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; display: inline-block">
          </div>
          <p> My selected publications are listed here. The complete list of publications can be seen from my <a href="https://scholar.google.com/citations?user=GW9vw8UAAAAJ&hl=en">Google Scholar</a> page.</p>
          <p>^ students mentored by me. * equal contribution</p>
      </td>
      </tr>
      </table>

	  
	 <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


    <tr onmouseout="topformer_stop()" onmouseover="topformer_start()">
            <td style="padding:20px;width:25%;vertical-align:middle;">
              <div class="one">
                <div class="two" id='topformer_image'><img src='resource/superclass.png' width="160" height="120" style="border-style: none"></div>
                <img src='resource/superclass.png' width="160" height="120" style="border-style: none">
              <script type="text/javascript">
                function diffposenet_start() {
                  document.getElementById('topformer_image').style.opacity = "1";
                }

                function diffposenet_stop() {
                  document.getElementById('topformer_image').style.opacity = "0";
                }
                diffposenet_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://github.com/x-cls/superclass">
                <papertitle>Classification Done Right for Vision-Language Pre-Training</papertitle>
              </a>
              <br>
              <strong>Zilong Huang</strong>, Qinghao Ye, Bingyi Kang, Jiashi Feng, Haoqi Fan
              <br>
        <em><a href="">NeurIPS, 2024</a></em> 
              <br>
              <a href="https://github.com/x-cls/superclass">code</a> /
              <a href="https://arxiv.org/abs/2411.03313">pdf</a>
        
              <p></p>
              <p style="color:rgb(110, 110, 110)"> We introduce SuperClass, a super simple classification method for vision-language pre-training on image-text data.</p>
            </td>
          </tr>


     <tr onmouseout="topformer_stop()" onmouseover="topformer_start()">
            <td style="padding:20px;width:25%;vertical-align:middle;">
              <div class="one">
                <div class="two" id='topformer_image'><img src='resource/depth_anythingv2.png' width="160" height="120" style="border-style: none"></div>
                <img src='resource/depth_anythingv2.png' width="160" height="120" style="border-style: none">
              <script type="text/javascript">
                function diffposenet_start() {
                  document.getElementById('topformer_image').style.opacity = "1";
                }

                function diffposenet_stop() {
                  document.getElementById('topformer_image').style.opacity = "0";
                }
                diffposenet_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://depth-anything-v2.github.io">
                <papertitle>Depth Anything V2</papertitle>
              </a>
              <br>
              Lihe Yang, Bingyi Kang, <strong>Zilong Huang</strong>, Zhen Zhao, Xiaogang Xu, Jiashi Feng, Hengshuang Zhao
              <br>
        <em><a href="">NeurIPS, 2024</a></em> 
              <br>
              <a href="https://github.com/DepthAnything/Depth-Anything-V2">code</a> /
              <a href="https://arxiv.org/abs/2406.09414">pdf</a>
        
              <p></p>
              <p style="color:rgb(110, 110, 110)"> Depth Anything V2 is trained from 595K synthetic labeled images and 62M+ real unlabeled images.</p>
            </td>
          </tr>


    <tr onmouseout="topformer_stop()" onmouseover="topformer_start()">
            <td style="padding:20px;width:25%;vertical-align:middle;">
              <div class="one">
                <div class="two" id='topformer_image'><img src='resource/dig.png' width="160" height="120" style="border-style: none"></div>
                <img src='resource/dig.png' width="160" height="120" style="border-style: none">
              <script type="text/javascript">
                function diffposenet_start() {
                  document.getElementById('topformer_image').style.opacity = "1";
                }

                function diffposenet_stop() {
                  document.getElementById('topformer_image').style.opacity = "0";
                }
                diffposenet_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2405.18428">
                <papertitle>DiG: Scalable and Efficient Diffusion Models with Gated Linear Attention</papertitle>
              </a>
              <br>
              Lianghui Zhu, <strong>Zilong Huang</strong>, Bencheng Liao, Jun Hao Liew, Hanshu Yan, Jiashi Feng, Xinggang Wang
              <br>
        <em><a href="">Arxiv, 2024</a></em> 
              <br>
              <a href="https://github.com/hustvl/DiG">code</a> /
              <a href="https://arxiv.org/abs/2405.18428">pdf</a>
        
              <p></p>
              <p style="color:rgb(110, 110, 110)"> This work presents Diffusion GLA, the first exploration for diffusion backbone with linear attention
transformer.</p>
            </td>
          </tr>

    <tr onmouseout="topformer_stop()" onmouseover="topformer_start()">
            <td style="padding:20px;width:25%;vertical-align:middle;">
              <div class="one">
                <div class="two" id='topformer_image'><img src='resource/meta_prompt.png' width="160" height="120" style="border-style: none"></div>
                <img src='resource/meta_prompt.png' width="160" height="120" style="border-style: none">
              <script type="text/javascript">
                function diffposenet_start() {
                  document.getElementById('topformer_image').style.opacity = "1";
                }

                function diffposenet_stop() {
                  document.getElementById('topformer_image').style.opacity = "0";
                }
                diffposenet_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2304.00784">
                <papertitle>Harnessing Diffusion Models for Visual Perception with Meta Prompts</papertitle>
              </a>
              <br>
              Qiang Wan, <strong>Zilong Huang</strong>, Bingyi Kang, Jiashi Feng, Li Zhang.
              <br>
        <em><a href="">Arxiv, 2024</a></em> 
              <br>
              <a href="https://github.com/fudan-zvg/meta-prompts">code</a> /
              <a href="https://arxiv.org/pdf/2312.14733v1.pdf">pdf</a>
        
              <p></p>
              <p style="color:rgb(110, 110, 110)"> This work presents Meta Prompts, a simple yet effective scheme to harness a diffusion model for visual perception tasks.</p>
            </td>
          </tr>

    <tr onmouseout="topformer_stop()" onmouseover="topformer_start()">
            <td style="padding:20px;width:25%;vertical-align:middle;">
              <div class="one">
                <div class="two" id='topformer_image'><img src='resource/depth_anything.png' width="160" height="120" style="border-style: none"></div>
                <img src='resource/depth_anything.png' width="160" height="120" style="border-style: none">
              <script type="text/javascript">
                function diffposenet_start() {
                  document.getElementById('topformer_image').style.opacity = "1";
                }

                function diffposenet_stop() {
                  document.getElementById('topformer_image').style.opacity = "0";
                }
                diffposenet_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data</papertitle>
              </a>
              <br>
              Lihe Yang, Bingyi Kang, <strong>Zilong Huang</strong>, Xiaogang Xu, Jiashi Feng, Hengshuang Zhao.
              <br>
        <em><a href="https://arxiv.org/abs/2304.00784">CVPR, 2024</a></em> 
              <br>
              <a href="https://github.com/LiheYoung/Depth-Anything">code</a> /
              <a href="https://arxiv.org/abs/2401.10891">pdf</a>
        
              <p></p>
              <!-- <p style="color:rgb(255, 0, 0)"> The most popular depth.</p> -->
              <p style="color:rgb(110, 110, 110)"> This work presents Depth Anything, a highly practical solution for robust monocular depth estimation by training on a combination of 1.5M labeled images and 62M+ unlabeled images.</p>
            </td>
          </tr>

    <tr onmouseout="topformer_stop()" onmouseover="topformer_start()">
            <td style="padding:20px;width:25%;vertical-align:middle;">
              <div class="one">
                <div class="two" id='topformer_image'><img src='resource/dpt.png' width="160" height="120" style="border-style: none"></div>
                <img src='resource/dpt.png' width="160" height="120" style="border-style: none">
              <script type="text/javascript">
                function diffposenet_start() {
                  document.getElementById('topformer_image').style.opacity = "1";
                }

                function diffposenet_stop() {
                  document.getElementById('topformer_image').style.opacity = "0";
                }
                diffposenet_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2304.00784">
                <papertitle>Disentangled Pre-training for Image Matting</papertitle>
              </a>
              <br>
              Yanda Li^, <strong>Zilong Huang</strong>, Gang Yu, Ling Chen, Yunchao Wei, Jianbo Jiao
              <br>
        <em><a href="https://arxiv.org/abs/2304.00784">WACV, 2024</a></em> 
              <br>
              <a href="https://github.com/crystraldo/dpt">code</a> /
              <a href="https://arxiv.org/pdf/2304.00784.pdf">pdf</a>
        
              <p></p>
              <p style="color:rgb(110, 110, 110)"> we propose the first self-supervised large-scale pretraining approach for image matting.</p>
            </td>
          </tr>


    <tr onmouseout="topformer_stop()" onmouseover="topformer_start()">
            <td style="padding:20px;width:25%;vertical-align:middle;">
              <div class="one">
                <div class="two" id='topformer_image'><img src='resource/mld.png' width="160" height="120" style="border-style: none"></div>
                <img src='resource/mld.png' width="160" height="120" style="border-style: none">
              <script type="text/javascript">
                function diffposenet_start() {
                  document.getElementById('topformer_image').style.opacity = "1";
                }

                function diffposenet_stop() {
                  document.getElementById('topformer_image').style.opacity = "0";
                }
                diffposenet_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2210.11170">
                <papertitle>Executing your Commands via Motion Diffusion in Latent Space</papertitle>
              </a>
              <br>
              Xin Chen*, Biao Jiang*, Wen Liu, <strong>Zilong Huang</strong>, Bin Fu, Tao Chen, Jingyi Yu, Gang Yu
              <br>
        <em><a href="https://iclr.cc/">CVPR, 2023</a></em> 
              <br>
              <a href="https://github.com/chenfengye/motion-latent-diffusion">code</a> /
              <a href="https://arxiv.org/pdf/2212.04048.pdf">pdf</a>
        
              <p></p>
              <p style="color:rgb(110, 110, 110)"> we propose a Motion Latent-based Diffusion model (MLD) could produce vivid motion sequences conforming to the given conditional inputs.</p>
            </td>
          </tr> 

    <tr onmouseout="topformer_stop()" onmouseover="topformer_start()">
            <td style="padding:20px;width:25%;vertical-align:middle;">
              <div class="one">
                <div class="two" id='topformer_image'><img src='resource/seaformer.png' width="160" height="120" style="border-style: none"></div>
                <img src='resource/seaformer.png' width="160" height="120" style="border-style: none">
              <script type="text/javascript">
                function diffposenet_start() {
                  document.getElementById('topformer_image').style.opacity = "1";
                }

                function diffposenet_stop() {
                  document.getElementById('topformer_image').style.opacity = "0";
                }
                diffposenet_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2210.11170">
                <papertitle>SeaFormer: Squeeze-enhanced Axial Transformer for Mobile Semantic Segmentation</papertitle>
              </a>
              <br>
              Qiang Wan^, <strong>Zilong Huang</strong>, Jiachen Lu, Gang Yu, Li Zhang
              <br>
        <em><a href="https://iclr.cc/">ICLR, 2023</a></em> 
              <br>
              <a href="https://github.com/fudan-zvg/seaformer">code</a> /
              <a href="https://arxiv.org/abs/2301.13156">pdf</a>
        
              <p></p>
              <p style="color:rgb(110, 110, 110)"> we design a generic attention block characterized by the formulation of squeeze Axial and detail enhancement for mobile vision.</p>
            </td>
          </tr> 

    <tr onmouseout="topformer_stop()" onmouseover="topformer_start()">
            <td style="padding:20px;width:25%;vertical-align:middle;">
              <div class="one">
                <div class="two" id='topformer_image'><img src='resource/coco-nerf.png' width="160" height="120" style="border-style: none"></div>
                <img src='resource/coco-nerf.png' width="160" height="120" style="border-style: none">
              <script type="text/javascript">
                function diffposenet_start() {
                  document.getElementById('topformer_image').style.opacity = "1";
                }

                function diffposenet_stop() {
                  document.getElementById('topformer_image').style.opacity = "0";
                }
                diffposenet_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2210.11170">
                <papertitle>Coordinates Are NOT Lonely - Codebook Prior Helps Implicit Neural 3D Representations</papertitle>
              </a>
              <br>
              Fukun Yin*, Wen Liu*, <strong>Zilong Huang</strong>, Pei Cheng, Tao Chen, Gang Yu
              <br>
        <em><a href="https://nips.cc/">NeurIPS, 2022</a></em> 
              <br>
              <a href="https://github.com/fukunyin/coco-nerf">code</a> /
              <a href="https://arxiv.org/abs/2210.11170">pdf</a>
        
              <p></p>
              <p style="color:rgb(110, 110, 110)"> CoCo-INR is a novel framework for implicit neural 3D representations, which builds a connection between each coordinate and the prior information.</p>
            </td>
          </tr> 
	
		 
		<tr onmouseout="topformer_stop()" onmouseover="topformer_start()">
            <td style="padding:20px;width:25%;vertical-align:middle;">
              <div class="one">
                <div class="two" id='topformer_image'><img src='resource/topformer-speed-iou.png' width="160" height="120" style="border-style: none"></div>
                <img src='resource/topformer-speed-iou.png' width="160" height="120" style="border-style: none">
              <script type="text/javascript">
                function diffposenet_start() {
                  document.getElementById('topformer_image').style.opacity = "1";
                }

                function diffposenet_stop() {
                  document.getElementById('topformer_image').style.opacity = "0";
                }
                diffposenet_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2204.05525">
                <papertitle>TopFormer: Token Pyramid Transformer for Mobile Semantic Segmentation</papertitle>
              </a>
              <br>
              Wenqiang Zhang^*, 
              <strong>Zilong Huang*</strong>,
              Guozhong Luo, 
              Tao Chen,
              Xinggang Wang, 
              Wenyu Liu, 
              Gang Yu, 
              Chunhua Shen
              <br>
        <em><a href=" https://cvpr2022.thecvf.com/">CVPR, 2022</a></em> 
              <br>
              <a href="https://github.com/hustvl/TopFormer">code</a> /
              <a href="https://arxiv.org/abs/2204.05525">pdf</a>
		    
              <p></p>
              <p style="color:rgb(110, 110, 110)"> Topformer is the first work that makes transformer real-time on mobile devices for segmentation tasks.</p>
            </td>
          </tr> 

  <tr onmouseout="shuffle_trans_stop()" onmouseover="shuffle_trans_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='shuffle_trans_image'>
                  <img src='resource/shuffle_trans-vis.png' width="160" height="120" style="border-style: none">
                </div>
                <img src='resource/shuffle_trans-vis.png' width="160" height="120" style="border-style: none">
              </div>
              <script type="text/javascript">
                function spikems_start() {
                  document.getElementById('shuffle_trans_image').style.opacity = "1";
                }

                function spikems_stop() {
                  document.getElementById('shuffle_trans_image').style.opacity = "0";
                }
                spikems_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2106.03650">
                <papertitle>Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer</papertitle>
              </a>
              <br>
              <strong>Zilong Huang</strong>,
              Youcheng Ben, Guozhong Luo, Pei Cheng, Gang Yu, Bin Fu
              <br>
        <em><a href="">arXiv, 2021</a></em> 
              <br>
              <a href="https://github.com/mulinmeng/Shuffle-Transformer">code</a> /
              <a href="https://arxiv.org/pdf/2106.03650.pdf">pdf</a>
        
              <p></p>
              <p style="color:rgb(110, 110, 110)"> we revisit the spatial shuffle as an efficient way to build connections among windows in window-based self-attention.</p>
            </td>
          </tr> 

	<tr onmouseout="alignseg_stop()" onmouseover="alignseg_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='alignseg_image'>
                  <img src='resource/alignseg-offsets.png' width="160" height="120" style="border-style: none">
                </div>
                <img src='resource/alignseg-offsets.png' width="160" height="120" style="border-style: none">
              </div>
              <script type="text/javascript">
                function spikems_start() {
                  document.getElementById('alignseg_image').style.opacity = "1";
                }

                function spikems_stop() {
                  document.getElementById('alignseg_image').style.opacity = "0";
                }
                spikems_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2003.00872">
                <papertitle>AlignSeg: Feature-Aligned Segmentation Networks</papertitle>
              </a>
              <br>
              <strong>Zilong Huang</strong>,
	             Yunchao Wei, Xinggang Wang, Wenyu Liu, Thomas S. Huang, Humphrey Shi		
              <br>
        <em><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">TPAMI, 2021</a></em> 
              <br>
              <a href="https://github.com/speedinghzl/AlignSeg">code</a> /
              <a href="https://arxiv.org/pdf/2003.00872">pdf</a>
		    
              <p></p>
              <p style="color:rgb(110, 110, 110)"> we focus on the feature misalignment issue in previous popular feature aggregation architectures for semantic segmentation.</p>
            </td>
          </tr> 

  <tr onmouseout="hude_stop()" onmouseover="hude_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='hude_image'>
                  <img src='resource/hude-vis.png' width="160" height="120" style="border-style: none">
                </div>
                <img src='resource/hude-vis.png' width="160" height="120" style="border-style: none">
              </div>
              <script type="text/javascript">
                function hude_start() {
                  document.getElementById('hude_image').style.opacity = "1";
                }

                function hude_stop() {
                  document.getElementById('hude_image').style.opacity = "0";
                }
                hude_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2106.03650">
                <papertitle>Human De-occlusion: Invisible Perception and Recovery for Humans</papertitle>
              </a>
              <br>
              Qiang Zhou, Shiyin Wang, Yitong Wang, <strong>Zilong Huang</strong>, Xinggang Wang
              <br>
        <em><a href="">CVPR, 2021</a></em> 
              <br>
              <a href="https://sydney0zq.github.io/ahp/">dataset</a> /
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhou_Human_De-Occlusion_Invisible_Perception_and_Recovery_for_Humans_CVPR_2021_paper.pdf">pdf</a>
        
              <p></p>
              <p style="color:rgb(110, 110, 110)"> we tackle the problem of human de-occlusion which reasons about occluded segmentation masks and invisible appearance content of humans.</p>
            </td>
          </tr> 

  <tr onmouseout="hrmatting_stop()" onmouseover="hrmatting_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='hrmatting_image'><img src='resource/hrmatting-vis.png' width="160" height="120" style="border-style: none"></div>
                <img src='resource/hrmatting-vis.png' width="160" height="120" style="border-style: none">
              </div>
              <script type="text/javascript">
                function hrmatting_start() {
                  document.getElementById('hrmatting_image').style.opacity = "1";
                }

                function hrmatting_stop() {
                  document.getElementById('hrmatting_image').style.opacity = "0";
                }
                hrmatting_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1908.09798.pdf">
                <papertitle>High-Resolution Deep Image Matting</papertitle>
              </a>
              <br>
              Haichao Yu, Ning Xu, <strong>Zilong Huang</strong>, Yuqian Zhou, Humphrey Shi.
              <br>
        <em><a href="">AAAI, 2021</a></em>
  
              <br>
              <a href="https://arxiv.org/pdf/1908.09798.pdf">pdf</a>
              <p></p>
              <p style="color:rgb(110, 110, 110)"> we propose HDMatt, a first deep learning based image matting approach for high-resolution inputs.</p>
            </td>
          </tr> 

  <tr onmouseout="agri_stop()" onmouseover="agri_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='agri_image'><img src='resource/agri-logo.png' width="160" height="130" style="border-style: none"></div>
                <img src='resource/agri-logo.png' width="160" height="130" style="border-style: none">
              </div>
              <script type="text/javascript">
                function agri_start() {
                  document.getElementById('agri_image').style.opacity = "1";
                }

                function agri_stop() {
                  document.getElementById('agri_image').style.opacity = "0";
                }
                agri_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.agriculture-vision.com">
                <papertitle>Agriculture-Vision: A Large Aerial Image Database for Agricultural Pattern Analysis</papertitle>
              </a>
              <br>
               Mang Tik Chiu*, Xingqian Xu*, Yunchao Wei, <strong>Zilong Huang</strong>, Alexander Schwing, Robert Brunner, Hrant Khachatrian, Hovnatan Karapetyan, Ivan Dozier, Greg Rose, David Wilson, Adrian Tudor, Naira Hovakimyan, Thomas S Huang, Honghui Shi
              <br>
          <em><a href="https://www.cvpr.org/">CVPR, 2020</a></em>  
              <br>
              <a href="https://www.agriculture-vision.com">dataset</a> /
              <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Chiu_Agriculture-Vision_A_Large_Aerial_Image_Database_for_Agricultural_Pattern_Analysis_CVPR_2020_paper.pdf">pdf</a> /
              <a href="https://www.youtube.com/watch?v=fcxU6CSVQfA">video</a> 
              <p></p>
              <p style="color:rgb(110, 110, 110)"> we present Agriculture-Vision: a large-scale aerial farmland image dataset for semantic segmentation of agricultural patterns.</p>
            </td>
          </tr>
		 
		 
	<tr onmouseout="ccnet_stop()" onmouseover="ccnet_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ccnet_image'><img src='resource/ccnet-logo.png' width="160" height="160" style="border-style: none"></div>
                <img src='resource/ccnet-logo.png' width="160" height="160" style="border-style: none">
              </div>
              <script type="text/javascript">
                function ccnet_start() {
                  document.getElementById('ccnet_image').style.opacity = "1";
                }

                function ccnet_stop() {
                  document.getElementById('ccnet_image').style.opacity = "0";
                }
                ccnet_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1811.11721">
                <papertitle>CCNet: Criss-Cross Attention for Semantic Segmentation</papertitle>
              </a>
              <br>
	             <strong>Zilong Huang</strong>, 
               Xinggang Wang, Yunchao Wei, Lichao Huang, Humphrey Shi, Wenyu Liu, Thomas S. Huang
              <br>
        	<em><a href="https://www.iccv.org/">ICCV, 2019</a> |<a href="https://www.pami.org/">TPAMI, 2020</a></em>  
              <br>
              <a href="https://github.com/speedinghzl/CCNet">code</a> /
              <a href="https://arxiv.org/pdf/1811.11721.pdf">pdf</a> 
              <p></p>
              <p style="color:rgb(255, 0, 0)"> <a href="https://www.paperdigest.org/2021/08/most-influential-iccv-papers-2021-08" style="color:#FF0000;">More than 3000 citations, PaperDigest Most Influential ICCV 2019 papers (5th)</a>. 
              Applications of CCNet also include <a href="https://www.nature.com/articles/s41586-021-03819-2" style="color:#FF0000;">AlphaFold2</a>.</p>
              <p style="color:rgb(110, 110, 110)"> we propose a Criss-Cross Network (CCNet) for obtaining full-image contextual information in a very effective and efficient way.</p>
            </td>
          </tr>

    <tr onmouseout="spgnet_stop()" onmouseover="spgnet_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='spgnet_image'><img src='resource/spgnet-vis.png' width="160" height="120" style="border-style: none"></div>
                <img src='resource/spgnet-vis.png' width="160" height="120" style="border-style: none">
              </div>
              <script type="text/javascript">
                function spgnet_start() {
                  document.getElementById('spgnet_image').style.opacity = "1";
                }

                function spgnet_stop() {
                  document.getElementById('spgnet_image').style.opacity = "0";
                }
                spgnet_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1908.09798.pdf">
                <papertitle>SPGNet: Semantic Prediction Guidance for Scene Parsing</papertitle>
              </a>
              <br>
              Bowen Cheng, Liang-Chieh Chen, Yunchao Wei, Yukun Zhu, <strong>Zilong Huang</strong>, Jinjun Xiong, Thomas Huang, Wen-Mei Hwu, Humphrey Shi.
              <br>
        <em><a href="">ICCV, 2019</a></em>
  
              <br>
              <a href="https://arxiv.org/pdf/1908.09798.pdf">pdf</a>
              <p></p>
              <p style="color:rgb(110, 110, 110)"> we propose a Semantic Prediction Guidance (SPG) module which learns to re-weight the
              local features through the guidance from pixel-wise semantic prediction.</p>
            </td>
          </tr> 

    <tr onmouseout="san_stop()" onmouseover="san_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='san_image'><img src='resource/san-arch.png' width="160" height="120" style="border-style: none"></div>
                <img src='resource/san-arch.png' width="160" height="120" style="border-style: none">
              </div>
              <script type="text/javascript">
                function san_start() {
                  document.getElementById('san_image').style.opacity = "1";
                }

                function san_stop() {
                  document.getElementById('san_image').style.opacity = "0";
                }
                san_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/8879686">
                <papertitle>Semantic Image Segmentation by Scale-Adaptive Networks</papertitle>
              </a>
              <br>
              <strong>Zilong Huang</strong>, 
              Chunyu Wang, Xinggang Wang, Wenyu Liu, Jingdong Wang
              <br>
          <em><a href="https://roboticsconference.org/">TIP, 2019</a></em>  
              <br>
              <a href="https://github.com/speedinghzl/Scale-Adaptive-Network">code</a> /
              <a href="resource/san.pdf">pdf</a> 
              <p></p>
              <p style="color:rgb(110, 110, 110)"> we propose a Scale-Adaptive Network (SAN) which consists of multiple branches with each one taking charge of the segmentation of the objects of a certain range of scales.</p>
            </td>
          </tr>  


    <tr onmouseout="ce2p_stop()" onmouseover="ce2p_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ce2p_image'><img src='resource/ce2p-vis.png' width="160" height="120" style="border-style: none"></div>
                <img src='resource/ce2p-vis.png' width="160" height="120" style="border-style: none">
              </div>
              <script type="text/javascript">
                function ce2p_start() {
                  document.getElementById('ce2p_image').style.opacity = "1";
                }

                function ce2p_stop() {
                  document.getElementById('ce2p_image').style.opacity = "0";
                }
                ce2p_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1809.05996">
                <papertitle>Devil in the Details: Towards Accurate Single and Multiple Human Parsing</papertitle>
              </a>
              <br>
              
              Tao Ruan*, Ting Liu*, <strong>Zilong Huang</strong>, Yunchao Wei, Shikui Wei, Yao Zhao, Thomas Huang
              <br>
        <em><a href="">AAAI, 2019</a></em>
  
              <br>
              <a href="https://github.com/liutinglt/CE2P">code</a> /
              <a href="https://arxiv.org/pdf/1809.05996.pdf">pdf</a>
              <p></p>
              <p style="color:rgb(110, 110, 110)"> we identify several useful properties, including feature resolution, global context information and edge details, and perform rigorous analyses to reveal how to leverage them to benefit the human parsing task.</p>
            </td>
          </tr>
		 
		 
	  <tr onmouseout="dsrg_stop()" onmouseover="dsrg_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dsrg_image'><img src='resource/dsrg-logo.png' width="160" height="120" style="border-style: none"></div>
                <img src='resource/dsrg-logo.png' width="160" height="120" style="border-style: none">
              </div>
              <script type="text/javascript">
                function dsrg_start() {
                  document.getElementById('dsrg_image').style.opacity = "1";
                }

                function dsrg_stop() {
                  document.getElementById('dsrg_image').style.opacity = "0";
                }
                dsrg_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_Weakly-Supervised_Semantic_Segmentation_CVPR_2018_paper.pdf">
                <papertitle>Weakly-supervised semantic segmentation network with deep seeded region growing</papertitle>
              </a>
              <br>
              <strong>Zilong Huang</strong>,
              Xinggang Wang, Jiasi Wang, Wenyu Liu, Jingdong Wang
              <br>
        	<em><a href="https://www.cvpr2018.org/">CVPR, 2018</a></em>  
              <br>
              <a href="https://github.com/speedinghzl/DSRG">code</a> /
              <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_Weakly-Supervised_Semantic_Segmentation_CVPR_2018_paper.pdf">pdf</a> 
              <p></p>
              <p style="color:rgb(110, 110, 110)"> we propose to train a semantic segmentation network starting from the discriminative regions and
              progressively increase the pixel-level supervision using by seeded region growing.</p>
            </td>
          </tr> 

      
	
	    
	 <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		 
	   <!--tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="resource/betterflow.PNG" alt="b3do" width="160" style="border-style: none">
            </td-->


<!-- Footer -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <br>
              <p align="right">
                <font size="2">
		  
                 Last updated on March 09, 2022. Thanks to <a href="https://people.eecs.berkeley.edu/~barron/">Jon Barron</a> for this minimalist website template.
                </font>
              </p>
            </td>
          </tr>
        </table>        
        </td>
      </tr>
    </table>
    <script xml:space="preserve" language="JavaScript">
      hideallbibs();
    </script>
    <script xml:space="preserve" language="JavaScript">
      hideblock('3dpaper_abs');
    </script>
    <script xml:space="preserve" language="JavaScript">
      hideblock('rwfm_abs');
    </script>
    <script xml:space="preserve" language="JavaScript">
      hideblock('ads_abs');
    </script>
    <script>
  // (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  // (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  // m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  // })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  // ga('create', 'UA-102204172-1', 'auto');
  // ga('send', 'pageview');

</script>
    <script>
  

</script>
  </body>
</html>
